{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pb1ZUrtWDPwK"},"outputs":[],"source":["from datetime import datetime\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n","\n","!ncu \\\n","  --set full \\\n","  --force-overwrite \\\n","  -o colab_ncu_{timestamp} \\\n","  ./wmma_matrixMul2 1024 2048 1024 > output_ncu_{timestamp}.txt\n","\n","from datetime import datetime\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n","\n","!ncu \\\n","  --set full \\\n","  --force-overwrite \\\n","  -o colab_ncu_{timestamp} \\\n","  ./wmma_matrixMul4 1024 2048 1024 > output_ncu_{timestamp}.txt\n","\n","#RUN PROFILING NCU at FULL on A100\n","from datetime import datetime\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n","\n","!ncu \\\n","  --set full \\\n","  --force-overwrite \\\n","  -o colab_ncu_{timestamp} \\\n","  ./wmma_matrixMul8 1024 2048 1024 > output_ncu_{timestamp}.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wtxYG5SmujIr","outputId":"94019fdc-c17e-4cd5-9481-cecb2a432a89"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix dimensions: A(8192 x 2048) * B(2048 x 8192) = C(8192 x 8192)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 512 fragments (covering 8192 elements)\n","  N direction: 512 fragments (covering 8192 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 33554432\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 1538583.501 ms\n","\n","==PROF== Connected to process 849 (/content/wmma_matrixMul2)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%..\n","==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n","..50%....100% - 6 passes\n","GEMM time: 18730.105 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (256, 512), Block: (64, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 1546.093 ms\n","WMMA average relative error: 0.0000189239\n","DEBUG - First 10 values:\n","  CPU:  50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  GEMM: 50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  WMMA: 50408.11 51127.55 48970.34 50827.61 50106.71 49923.45 50918.34 51456.10 51410.02 50586.02 \n","\n","=== Performance Summary ===\n","CPU:  1538583.501 ms\n","GEMM: 18730.105 ms (82.14x speedup over CPU)\n","WMMA: 1546.093 ms (995.14x speedup over CPU, 12.11x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000189239\n","==PROF== Disconnected from process 849\n","[849] wmma_matrixMul2@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (512, 512, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.72\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.70\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.74\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (256, 512, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- -------------\n","    Metric Name                                              Metric Unit  Metric Value\n","    -------------------------------------------------------- ----------- -------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                         0\n","    sm__pipe_shared_cycles_active.avg                              cycle 27,564,026.60\n","    sm__pipe_shared_cycles_active.max                              cycle    27,996,316\n","    sm__pipe_shared_cycles_active.min                              cycle    27,102,060\n","    sm__pipe_shared_cycles_active.sum                              cycle 1,102,561,064\n","    sm__pipe_tensor_cycles_active.avg                              cycle 27,564,026.60\n","    sm__pipe_tensor_cycles_active.max                              cycle    27,996,316\n","    sm__pipe_tensor_cycles_active.min                              cycle    27,102,060\n","    sm__pipe_tensor_cycles_active.sum                              cycle 1,102,561,064\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %         17.62\n","    -------------------------------------------------------- ----------- -------------\n","\n","  float_to_double(const float *, double *, int) (262144, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        41.46\n","    -------------------------------------------------------- ----------- ------------\n","\n","Matrix dimensions: A(8192 x 2048) * B(2048 x 8192) = C(8192 x 8192)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 512 fragments (covering 8192 elements)\n","  N direction: 512 fragments (covering 8192 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 33554432\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 1660147.695 ms\n","\n","==PROF== Connected to process 7275 (/content/wmma_matrixMul4)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%..\n","==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n","..50%....100% - 6 passes\n","GEMM time: 18550.096 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (128, 512), Block: (128, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 1545.059 ms\n","WMMA average relative error: 0.0000189239\n","DEBUG - First 10 values:\n","  CPU:  50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  GEMM: 50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  WMMA: 50408.11 51127.55 48970.34 50827.61 50106.71 49923.45 50918.34 51456.10 51410.02 50586.02 \n","\n","=== Performance Summary ===\n","CPU:  1660147.695 ms\n","GEMM: 18550.096 ms (89.50x speedup over CPU)\n","WMMA: 1545.059 ms (1074.49x speedup over CPU, 12.01x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000189239\n","==PROF== Disconnected from process 7275\n","[7275] wmma_matrixMul4@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (512, 512, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.72\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.58\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.61\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (128, 512, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- -------------\n","    Metric Name                                              Metric Unit  Metric Value\n","    -------------------------------------------------------- ----------- -------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                         0\n","    sm__pipe_shared_cycles_active.avg                              cycle 27,561,554.60\n","    sm__pipe_shared_cycles_active.max                              cycle    27,909,762\n","    sm__pipe_shared_cycles_active.min                              cycle    27,191,669\n","    sm__pipe_shared_cycles_active.sum                              cycle 1,102,462,184\n","    sm__pipe_tensor_cycles_active.avg                              cycle 27,561,554.60\n","    sm__pipe_tensor_cycles_active.max                              cycle    27,909,762\n","    sm__pipe_tensor_cycles_active.min                              cycle    27,191,669\n","    sm__pipe_tensor_cycles_active.sum                              cycle 1,102,462,184\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %         17.62\n","    -------------------------------------------------------- ----------- -------------\n","\n","  float_to_double(const float *, double *, int) (262144, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        41.41\n","    -------------------------------------------------------- ----------- ------------\n","\n","Matrix dimensions: A(8192 x 2048) * B(2048 x 8192) = C(8192 x 8192)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 512 fragments (covering 8192 elements)\n","  N direction: 512 fragments (covering 8192 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 33554432\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 1601444.359 ms\n","\n","==PROF== Connected to process 14145 (/content/wmma_matrixMul8)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%..\n","==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n","..50%....100% - 6 passes\n","GEMM time: 18780.984 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (64, 512), Block: (256, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 1506.201 ms\n","WMMA average relative error: 0.0000189239\n","DEBUG - First 10 values:\n","  CPU:  50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  GEMM: 50408.34 51128.48 48970.92 50827.97 50107.96 49924.97 50918.91 51457.34 51411.45 50587.08 \n","  WMMA: 50408.11 51127.55 48970.34 50827.61 50106.71 49923.45 50918.34 51456.10 51410.02 50586.02 \n","\n","=== Performance Summary ===\n","CPU:  1601444.359 ms\n","GEMM: 18780.984 ms (85.27x speedup over CPU)\n","WMMA: 1506.201 ms (1063.23x speedup over CPU, 12.47x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000189239\n","==PROF== Disconnected from process 14145\n","[14145] wmma_matrixMul8@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (512, 512, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.72\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.71\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        54.66\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (64, 512, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- -------------\n","    Metric Name                                              Metric Unit  Metric Value\n","    -------------------------------------------------------- ----------- -------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                         0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                         0\n","    sm__pipe_shared_cycles_active.avg                              cycle 27,505,370.93\n","    sm__pipe_shared_cycles_active.max                              cycle    27,668,523\n","    sm__pipe_shared_cycles_active.min                              cycle    27,355,665\n","    sm__pipe_shared_cycles_active.sum                              cycle 1,100,214,837\n","    sm__pipe_tensor_cycles_active.avg                              cycle 27,505,370.93\n","    sm__pipe_tensor_cycles_active.max                              cycle    27,668,523\n","    sm__pipe_tensor_cycles_active.min                              cycle    27,355,665\n","    sm__pipe_tensor_cycles_active.sum                              cycle 1,100,214,837\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %         17.56\n","    -------------------------------------------------------- ----------- -------------\n","\n","  float_to_double(const float *, double *, int) (262144, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        41.41\n","    -------------------------------------------------------- ----------- ------------\n","\n"]}],"source":["!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul2 8192 2048 8192\n","!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul4 8192 2048 8192\n","!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul8 8192 2048 8192"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67566,"status":"ok","timestamp":1767103348558,"user":{"displayName":"Ade Pramono","userId":"09541809579049427775"},"user_tz":-60},"id":"GBwqe4kfjx0k","outputId":"36bf4a93-bb74-4635-88c9-c81ca56d1ed8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix dimensions: A(1024 x 2048) * B(2048 x 1024) = C(1024 x 1024)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 64 fragments (covering 1024 elements)\n","  N direction: 64 fragments (covering 1024 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 524288\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 18779.145 ms\n","\n","==PROF== Connected to process 9717 (/content/wmma_matrixMul2)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 6 passes\n","GEMM time: 764.281 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (32, 64), Block: (64, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 162.546 ms\n","WMMA average relative error: 0.0000190459\n","DEBUG - First 10 values:\n","  CPU:  52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  GEMM: 52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  WMMA: 52307.10 49624.96 50559.78 51349.49 49920.66 50794.24 50554.74 51388.32 51268.85 51507.41 \n","\n","=== Performance Summary ===\n","CPU:  18779.145 ms\n","GEMM: 764.281 ms (24.57x speedup over CPU)\n","WMMA: 162.546 ms (115.53x speedup over CPU, 4.70x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000190459\n","==PROF== Disconnected from process 9717\n","[9717] wmma_matrixMul2@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.05\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.44\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.50\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (32, 64, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle      430,143\n","    sm__pipe_shared_cycles_active.max                              cycle      437,196\n","    sm__pipe_shared_cycles_active.min                              cycle      411,516\n","    sm__pipe_shared_cycles_active.sum                              cycle   17,205,720\n","    sm__pipe_tensor_cycles_active.avg                              cycle      430,143\n","    sm__pipe_tensor_cycles_active.max                              cycle      437,196\n","    sm__pipe_tensor_cycles_active.min                              cycle      411,516\n","    sm__pipe_tensor_cycles_active.sum                              cycle   17,205,720\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        17.32\n","    -------------------------------------------------------- ----------- ------------\n","\n","  float_to_double(const float *, double *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        43.25\n","    -------------------------------------------------------- ----------- ------------\n","\n","Matrix dimensions: A(1024 x 2048) * B(2048 x 1024) = C(1024 x 1024)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 64 fragments (covering 1024 elements)\n","  N direction: 64 fragments (covering 1024 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 524288\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 19831.756 ms\n","\n","==PROF== Connected to process 9865 (/content/wmma_matrixMul4)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 6 passes\n","GEMM time: 974.353 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (16, 64), Block: (128, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 201.362 ms\n","WMMA average relative error: 0.0000190459\n","DEBUG - First 10 values:\n","  CPU:  52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  GEMM: 52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  WMMA: 52307.10 49624.96 50559.78 51349.49 49920.66 50794.24 50554.74 51388.32 51268.85 51507.41 \n","\n","=== Performance Summary ===\n","CPU:  19831.756 ms\n","GEMM: 974.353 ms (20.35x speedup over CPU)\n","WMMA: 201.362 ms (98.49x speedup over CPU, 4.84x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000190459\n","==PROF== Disconnected from process 9865\n","[9865] wmma_matrixMul4@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.05\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.75\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.09\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (16, 64, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle   430,038.90\n","    sm__pipe_shared_cycles_active.max                              cycle      437,381\n","    sm__pipe_shared_cycles_active.min                              cycle      419,637\n","    sm__pipe_shared_cycles_active.sum                              cycle   17,201,556\n","    sm__pipe_tensor_cycles_active.avg                              cycle   430,038.90\n","    sm__pipe_tensor_cycles_active.max                              cycle      437,381\n","    sm__pipe_tensor_cycles_active.min                              cycle      419,637\n","    sm__pipe_tensor_cycles_active.sum                              cycle   17,201,556\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        17.32\n","    -------------------------------------------------------- ----------- ------------\n","\n","  float_to_double(const float *, double *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        43.17\n","    -------------------------------------------------------- ----------- ------------\n","\n","Matrix dimensions: A(1024 x 2048) * B(2048 x 1024) = C(1024 x 1024)\n","\n","\n","=== WMMA Configuration ===\n","Fragment dimensions: M=16, N=16, K=16\n","Number of fragments:\n","  M direction: 64 fragments (covering 1024 elements)\n","  N direction: 64 fragments (covering 1024 elements)\n","  K direction: 128 fragments (covering 2048 elements)\n","  Total fragments used: 524288\n","  Fragment type: half precision (FP16) input, float accumulator\n","==========================\n","\n","Running CPU reference...\n","CPU time: 20310.073 ms\n","\n","==PROF== Connected to process 10027 (/content/wmma_matrixMul8)\n","Running tiled GEMM kernel...\n","==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 6 passes\n","GEMM time: 743.144 ms\n","GEMM average relative error: 0.0000000000\n","\n","==PROF== Profiling \"double_to_half\" - 1: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"double_to_half\" - 2: 0%....50%....100% - 6 passes\n","Running WMMA Tensor Core kernel...\n","WMMA Grid: (8, 64), Block: (256, 1)\n","==PROF== Profiling \"wmma_gemm\" - 3: 0%....50%....100% - 6 passes\n","==PROF== Profiling \"float_to_double\" - 4: 0%....50%....100% - 6 passes\n","WMMA time: 163.340 ms\n","WMMA average relative error: 0.0000190459\n","DEBUG - First 10 values:\n","  CPU:  52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  GEMM: 52308.02 49625.65 50560.00 51350.02 49922.43 50795.36 50556.24 51388.83 51269.56 51508.68 \n","  WMMA: 52307.10 49624.96 50559.78 51349.49 49920.66 50794.24 50554.74 51388.32 51268.85 51507.41 \n","\n","=== Performance Summary ===\n","CPU:  20310.073 ms\n","GEMM: 743.144 ms (27.33x speedup over CPU)\n","WMMA: 163.340 ms (124.34x speedup over CPU, 4.55x over GEMM)\n","\n","=== Accuracy Summary ===\n","GEMM avg relative error: 0.0000000000\n","WMMA avg relative error: 0.0000190459\n","==PROF== Disconnected from process 10027\n","[10027] wmma_matrixMul8@127.0.0.1\n","  gemm(const double *, const double *, double *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        97.06\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.94\n","    -------------------------------------------------------- ----------- ------------\n","\n","  double_to_half(const double *, __half *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        53.36\n","    -------------------------------------------------------- ----------- ------------\n","\n","  wmma_gemm(const __half *, const __half *, float *, int, int, int) (8, 64, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle   430,013.40\n","    sm__pipe_shared_cycles_active.max                              cycle      437,388\n","    sm__pipe_shared_cycles_active.min                              cycle      402,679\n","    sm__pipe_shared_cycles_active.sum                              cycle   17,200,536\n","    sm__pipe_tensor_cycles_active.avg                              cycle   430,013.40\n","    sm__pipe_tensor_cycles_active.max                              cycle      437,388\n","    sm__pipe_tensor_cycles_active.min                              cycle      402,679\n","    sm__pipe_tensor_cycles_active.sum                              cycle   17,200,536\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        17.35\n","    -------------------------------------------------------- ----------- ------------\n","\n","  float_to_double(const float *, double *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------------- ----------- ------------\n","    Metric Name                                              Metric Unit Metric Value\n","    -------------------------------------------------------- ----------- ------------\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.avg                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.max                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.min                        0\n","    l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.sum                        0\n","    sm__pipe_shared_cycles_active.avg                              cycle            0\n","    sm__pipe_shared_cycles_active.max                              cycle            0\n","    sm__pipe_shared_cycles_active.min                              cycle            0\n","    sm__pipe_shared_cycles_active.sum                              cycle            0\n","    sm__pipe_tensor_cycles_active.avg                              cycle            0\n","    sm__pipe_tensor_cycles_active.max                              cycle            0\n","    sm__pipe_tensor_cycles_active.min                              cycle            0\n","    sm__pipe_tensor_cycles_active.sum                              cycle            0\n","    sm__throughput.avg.pct_of_peak_sustained_elapsed                   %        44.10\n","    -------------------------------------------------------- ----------- ------------\n","\n"]}],"source":["!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul2 1024 2048 1024\n","!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul4 1024 2048 1024\n","!ncu --metrics sm__pipe_tensor_cycles_active,sm__pipe_shared_cycles_active,l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,sm__throughput.avg.pct_of_peak_sustained_elapsed ./wmma_matrixMul8 1024 2048 1024"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12801,"status":"ok","timestamp":1767106164062,"user":{"displayName":"Ade Pramono","userId":"09541809579049427775"},"user_tz":-60},"id":"Ksw1wuQG8L2b","outputId":"51cd2be6-e8a9-4c4d-dea1-a4781328a4d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Detected Architecture: sm_75\n"]}],"source":["import torch\n","\n","major, minor = torch.cuda.get_device_capability()\n","\n","arch_flag = f\"sm_{major}{minor}\"\n","print(f\"Detected Architecture: {arch_flag}\")\n","\n","!nvcc -arch={arch_flag} -O3 -o wmma_matrixMul2 wmma_matrixMul2.cu\n","!nvcc -arch={arch_flag} -O3 -o wmma_matrixMul4 wmma_matrixMul4.cu\n","!nvcc -arch={arch_flag} -O3 -o wmma_matrixMul8 wmma_matrixMul8.cu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1767106105631,"user":{"displayName":"Ade Pramono","userId":"09541809579049427775"},"user_tz":-60},"id":"8_DGyAH1BhMP","outputId":"9b4e6157-cd09-44da-a70a-2ee91d197907"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing wmma_matrixMul2.cu\n"]}],"source":["%%writefile wmma_matrixMul2.cu\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","#include <cuda_fp16.h>\n","#include <mma.h>\n","#include <chrono>\n","#include <cmath>\n","\n","using namespace nvcuda;\n","\n","#define CHECK(call) do {                                 \\\n","    cudaError_t err = (call);                            \\\n","    if (err != cudaSuccess) {                            \\\n","        fprintf(stderr, \"CUDA error: %s (%s:%d)\\n\",      \\\n","                cudaGetErrorString(err), __FILE__, __LINE__); \\\n","        exit(1);                                         \\\n","    }                                                    \\\n","} while (0)\n","\n","// WMMA tile dimensions (for half precision: 16x16x16)\n","#define WMMA_M 16\n","#define WMMA_N 16\n","#define WMMA_K 16\n","\n","// Tiled GEMM kernel (baseline GPU implementation)\n","#define TILE_SIZE 16\n","\n","__global__ void gemm(const double *A, const double *B, double *C,\n","                     int M, int K, int N) {\n","    __shared__ double As[TILE_SIZE][TILE_SIZE];\n","    __shared__ double Bs[TILE_SIZE][TILE_SIZE];\n","\n","    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n","    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n","\n","    double sum = 0.0;\n","\n","    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n","        // Load tiles into shared memory\n","        if (row < M && t * TILE_SIZE + threadIdx.x < K)\n","            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n","        else\n","            As[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        if (col < N && t * TILE_SIZE + threadIdx.y < K)\n","            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n","        else\n","            Bs[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        __syncthreads();\n","\n","        // Compute partial product\n","        for (int k = 0; k < TILE_SIZE; k++) {\n","            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (row < M && col < N) {\n","        C[row * N + col] = sum;\n","    }\n","}\n","\n","// Convert double precision to half precision\n","__global__ void double_to_half(const double *input, half *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = __float2half((float)input[idx]);\n","    }\n","}\n","\n","// Convert half precision to double precision\n","__global__ void half_to_double(const half *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)__half2float(input[idx]);\n","    }\n","}\n","\n","// WMMA kernel for matrix multiplication using Tensor Cores\n","// C (M x N) = A (M x K) * B (K x N)\n","// Using half precision for input and accumulation in float\n","// Output is float (will be converted to double later if needed)\n","__global__ void wmma_gemm(const half *A, const half *B, float *C,\n","                          int M, int K, int N) {\n","    // Leading dimensions\n","    int lda = K;\n","    int ldb = N;\n","    int ldc = N;\n","\n","    // Each warp computes one 16x16 output tile\n","    // Warp ID within the grid (each warp is an independent unit)\n","    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n","    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n","\n","    // Calculate the starting row and column for this warp's output tile\n","    int warp_row = warpM * WMMA_M;\n","    int warp_col = warpN * WMMA_N;\n","\n","    // Bounds check\n","    if (warp_row >= M || warp_col >= N)\n","        return;\n","\n","    // Declare the fragments\n","    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n","    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n","    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n","\n","    // Initialize accumulator to zero\n","    wmma::fill_fragment(acc_frag, 0.0f);\n","\n","    // Loop over K dimension in steps of WMMA_K\n","    for (int i = 0; i < K; i += WMMA_K) {\n","        if (i < K) {\n","            // Step 1: Load the inputs into fragments\n","            wmma::load_matrix_sync(a_frag, A + warp_row * lda + i, lda);\n","            wmma::load_matrix_sync(b_frag, B + i * ldb + warp_col, ldb);\n","\n","            // Step 2: Perform the matrix multiplication\n","            wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n","        }\n","    }\n","\n","    // Step 3: Store the output\n","    wmma::store_matrix_sync(C + warp_row * ldc + warp_col, acc_frag, ldc, wmma::mem_row_major);\n","}\n","\n","// Convert float to double\n","__global__ void float_to_double(const float *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)input[idx];\n","    }\n","}\n","\n","// CPU reference implementation\n","void cpu_gemm(const double *A, const double *B, double *C, int M, int K, int N) {\n","    for (int i = 0; i < M; i++) {\n","        for (int j = 0; j < N; j++) {\n","            double sum = 0.0;\n","            for (int k = 0; k < K; k++) {\n","                sum += A[i * K + k] * B[k * N + j];\n","            }\n","            C[i * N + j] = sum;\n","        }\n","    }\n","}\n","\n","// Calculate relative error\n","double calculate_error(const double *ref, const double *result, int size) {\n","    double max_error = 0.0;\n","    double sum_error = 0.0;\n","\n","    for (int i = 0; i < size; i++) {\n","        double error = fabs(ref[i] - result[i]);\n","        double rel_error = error / (fabs(ref[i]) + 1e-10);\n","        max_error = fmax(max_error, rel_error);\n","        sum_error += rel_error;\n","    }\n","\n","    return sum_error / size;\n","}\n","\n","void print_wmma_info(int M, int K, int N) {\n","    printf(\"\\n=== WMMA Configuration ===\\n\");\n","    printf(\"Fragment dimensions: M=%d, N=%d, K=%d\\n\", WMMA_M, WMMA_N, WMMA_K);\n","\n","    // Calculate number of fragments needed\n","    int frags_M = (M + WMMA_M - 1) / WMMA_M;\n","    int frags_N = (N + WMMA_N - 1) / WMMA_N;\n","    int frags_K = (K + WMMA_K - 1) / WMMA_K;\n","\n","    printf(\"Number of fragments:\\n\");\n","    printf(\"  M direction: %d fragments (covering %d elements)\\n\", frags_M, M);\n","    printf(\"  N direction: %d fragments (covering %d elements)\\n\", frags_N, N);\n","    printf(\"  K direction: %d fragments (covering %d elements)\\n\", frags_K, K);\n","    printf(\"  Total fragments used: %d\\n\", frags_M * frags_N * frags_K);\n","    printf(\"  Fragment type: half precision (FP16) input, float accumulator\\n\");\n","    printf(\"==========================\\n\\n\");\n","}\n","\n","int main(int argc, char **argv) {\n","    if (argc < 2) {\n","        printf(\"Usage: %s <size> or %s <M> <K> <N>\\n\", argv[0], argv[0]);\n","        printf(\"Example: %s 1024 2048 1024  (for A: 1024x2048, B: 2048x1024)\\n\", argv[0]);\n","        return 1;\n","    }\n","\n","    int M, K, N;\n","    if (argc == 2) {\n","        // Square matrices\n","        M = K = N = atoi(argv[1]);\n","    } else if (argc == 4) {\n","        M = atoi(argv[1]);\n","        K = atoi(argv[2]);\n","        N = atoi(argv[3]);\n","    } else {\n","        fprintf(stderr, \"Invalid arguments\\n\");\n","        return 1;\n","    }\n","\n","    printf(\"Matrix dimensions: A(%d x %d) * B(%d x %d) = C(%d x %d)\\n\\n\",\n","           M, K, K, N, M, N);\n","\n","    // Print WMMA information\n","    print_wmma_info(M, K, N);\n","\n","    // Allocate host memory\n","    size_t bytes_A = M * K * sizeof(double);\n","    size_t bytes_B = K * N * sizeof(double);\n","    size_t bytes_C = M * N * sizeof(double);\n","\n","    double *h_A = (double*)malloc(bytes_A);\n","    double *h_B = (double*)malloc(bytes_B);\n","    double *h_C_cpu = (double*)malloc(bytes_C);\n","    double *h_C_gemm = (double*)malloc(bytes_C);\n","    double *h_C_wmma = (double*)malloc(bytes_C);\n","\n","    // Initialize matrices\n","    for (int i = 0; i < M * K; i++) {\n","        h_A[i] = (double)(rand() % 100) / 10.0;\n","    }\n","    for (int i = 0; i < K * N; i++) {\n","        h_B[i] = (double)(rand() % 100) / 10.0;\n","    }\n","\n","    // CPU computation\n","    printf(\"Running CPU reference...\\n\");\n","    auto cpu_start = std::chrono::high_resolution_clock::now();\n","    cpu_gemm(h_A, h_B, h_C_cpu, M, K, N);\n","    auto cpu_end = std::chrono::high_resolution_clock::now();\n","    std::chrono::duration<double> cpu_duration = cpu_end - cpu_start;\n","    printf(\"CPU time: %.3f ms\\n\\n\", cpu_duration.count() * 1000.0);\n","\n","    // Allocate device memory for double precision\n","    double *d_A, *d_B, *d_C_gemm;\n","    CHECK(cudaMalloc(&d_A, bytes_A));\n","    CHECK(cudaMalloc(&d_B, bytes_B));\n","    CHECK(cudaMalloc(&d_C_gemm, bytes_C));\n","\n","    CHECK(cudaMemcpy(d_A, h_A, bytes_A, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_B, h_B, bytes_B, cudaMemcpyHostToDevice));\n","\n","    // Run tiled GEMM kernel\n","    printf(\"Running tiled GEMM kernel...\\n\");\n","    dim3 block_gemm(TILE_SIZE, TILE_SIZE);\n","    dim3 grid_gemm((N + TILE_SIZE - 1) / TILE_SIZE,\n","                   (M + TILE_SIZE - 1) / TILE_SIZE);\n","\n","    cudaEvent_t start, stop;\n","    CHECK(cudaEventCreate(&start));\n","    CHECK(cudaEventCreate(&stop));\n","\n","    CHECK(cudaEventRecord(start));\n","    gemm<<<grid_gemm, block_gemm>>>(d_A, d_B, d_C_gemm, M, K, N);\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float gemm_time = 0;\n","    CHECK(cudaEventElapsedTime(&gemm_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_gemm, d_C_gemm, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double gemm_error = calculate_error(h_C_cpu, h_C_gemm, M * N);\n","    printf(\"GEMM time: %.3f ms\\n\", gemm_time);\n","    printf(\"GEMM average relative error: %.10f\\n\\n\", gemm_error);\n","\n","    // Allocate device memory for half precision (for WMMA)\n","    half *d_A_half, *d_B_half;\n","    float *d_C_wmma_float;  // WMMA outputs float\n","    double *d_C_wmma_double;  // For final conversion to double\n","    CHECK(cudaMalloc(&d_A_half, M * K * sizeof(half)));\n","    CHECK(cudaMalloc(&d_B_half, K * N * sizeof(half)));\n","    CHECK(cudaMalloc(&d_C_wmma_float, M * N * sizeof(float)));\n","    CHECK(cudaMalloc(&d_C_wmma_double, bytes_C));\n","\n","    // Convert double to half\n","    int threads = 256;\n","    int blocks_A = (M * K + threads - 1) / threads;\n","    int blocks_B = (K * N + threads - 1) / threads;\n","\n","    double_to_half<<<blocks_A, threads>>>(d_A, d_A_half, M * K);\n","    double_to_half<<<blocks_B, threads>>>(d_B, d_B_half, K * N);\n","    CHECK(cudaDeviceSynchronize());\n","\n","    // Run WMMA kernel\n","    printf(\"Running WMMA Tensor Core kernel...\\n\");\n","\n","    // For WMMA: each warp (32 threads) computes one 16x16 output tile\n","    // Use 128 threads per block (4 warps) for good occupancy\n","    // Block dimensions: 128 threads in X (4 warps), 1 in Y\n","    int warps_per_block_x = 2;  // 4 warps per block\n","    dim3 block_wmma(warps_per_block_x * 32, 1);  // 128 threads in X, 1 in Y\n","    dim3 grid_wmma(((M + WMMA_M - 1) / WMMA_M + warps_per_block_x - 1) / warps_per_block_x,\n","                   (N + WMMA_N - 1) / WMMA_N);\n","\n","    printf(\"WMMA Grid: (%d, %d), Block: (%d, %d)\\n\",\n","           grid_wmma.x, grid_wmma.y, block_wmma.x, block_wmma.y);\n","\n","    CHECK(cudaEventRecord(start));\n","    wmma_gemm<<<grid_wmma, block_wmma>>>(d_A_half, d_B_half, d_C_wmma_float, M, K, N);\n","    CHECK(cudaGetLastError());\n","\n","    // Convert float output to double for comparison\n","    int blocks_C = (M * N + threads - 1) / threads;\n","    float_to_double<<<blocks_C, threads>>>(d_C_wmma_float, d_C_wmma_double, M * N);\n","    CHECK(cudaGetLastError());\n","\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float wmma_time = 0;\n","    CHECK(cudaEventElapsedTime(&wmma_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_wmma, d_C_wmma_double, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double wmma_error = calculate_error(h_C_cpu, h_C_wmma, M * N);\n","    printf(\"WMMA time: %.3f ms\\n\", wmma_time);\n","    printf(\"WMMA average relative error: %.10f\\n\", wmma_error);\n","\n","    // Debug: Print first few values\n","    printf(\"DEBUG - First 10 values:\\n\");\n","    printf(\"  CPU:  \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_cpu[i]);\n","    printf(\"\\n  GEMM: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_gemm[i]);\n","    printf(\"\\n  WMMA: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_wmma[i]);\n","    printf(\"\\n\\n\");\n","\n","    // Print performance comparison\n","    printf(\"=== Performance Summary ===\\n\");\n","    printf(\"CPU:  %.3f ms\\n\", cpu_duration.count() * 1000.0);\n","    printf(\"GEMM: %.3f ms (%.2fx speedup over CPU)\\n\",\n","           gemm_time, cpu_duration.count() * 1000.0 / gemm_time);\n","    printf(\"WMMA: %.3f ms (%.2fx speedup over CPU, %.2fx over GEMM)\\n\",\n","           wmma_time, cpu_duration.count() * 1000.0 / wmma_time, gemm_time / wmma_time);\n","    printf(\"\\n=== Accuracy Summary ===\\n\");\n","    printf(\"GEMM avg relative error: %.10f\\n\", gemm_error);\n","    printf(\"WMMA avg relative error: %.10f\\n\", wmma_error);\n","\n","    // Cleanup\n","    CHECK(cudaEventDestroy(start));\n","    CHECK(cudaEventDestroy(stop));\n","    free(h_A);\n","    free(h_B);\n","    free(h_C_cpu);\n","    free(h_C_gemm);\n","    free(h_C_wmma);\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_B));\n","    CHECK(cudaFree(d_C_gemm));\n","    CHECK(cudaFree(d_A_half));\n","    CHECK(cudaFree(d_B_half));\n","    CHECK(cudaFree(d_C_wmma_float));\n","    CHECK(cudaFree(d_C_wmma_double));\n","\n","    return 0;\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82,"status":"ok","timestamp":1767106119598,"user":{"displayName":"Ade Pramono","userId":"09541809579049427775"},"user_tz":-60},"id":"GPNJv86rBnQJ","outputId":"6c610d4f-dccc-42d1-a7d0-22f89c954b7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing wmma_matrixMul4.cu\n"]}],"source":["%%writefile wmma_matrixMul4.cu\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","#include <cuda_fp16.h>\n","#include <mma.h>\n","#include <chrono>\n","#include <cmath>\n","\n","using namespace nvcuda;\n","\n","#define CHECK(call) do {                                 \\\n","    cudaError_t err = (call);                            \\\n","    if (err != cudaSuccess) {                            \\\n","        fprintf(stderr, \"CUDA error: %s (%s:%d)\\n\",      \\\n","                cudaGetErrorString(err), __FILE__, __LINE__); \\\n","        exit(1);                                         \\\n","    }                                                    \\\n","} while (0)\n","\n","// WMMA tile dimensions (for half precision: 16x16x16)\n","#define WMMA_M 16\n","#define WMMA_N 16\n","#define WMMA_K 16\n","\n","// Tiled GEMM kernel (baseline GPU implementation)\n","#define TILE_SIZE 16\n","\n","__global__ void gemm(const double *A, const double *B, double *C,\n","                     int M, int K, int N) {\n","    __shared__ double As[TILE_SIZE][TILE_SIZE];\n","    __shared__ double Bs[TILE_SIZE][TILE_SIZE];\n","\n","    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n","    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n","\n","    double sum = 0.0;\n","\n","    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n","        // Load tiles into shared memory\n","        if (row < M && t * TILE_SIZE + threadIdx.x < K)\n","            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n","        else\n","            As[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        if (col < N && t * TILE_SIZE + threadIdx.y < K)\n","            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n","        else\n","            Bs[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        __syncthreads();\n","\n","        // Compute partial product\n","        for (int k = 0; k < TILE_SIZE; k++) {\n","            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (row < M && col < N) {\n","        C[row * N + col] = sum;\n","    }\n","}\n","\n","// Convert double precision to half precision\n","__global__ void double_to_half(const double *input, half *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = __float2half((float)input[idx]);\n","    }\n","}\n","\n","// Convert half precision to double precision\n","__global__ void half_to_double(const half *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)__half2float(input[idx]);\n","    }\n","}\n","\n","// WMMA kernel for matrix multiplication using Tensor Cores\n","// C (M x N) = A (M x K) * B (K x N)\n","// Using half precision for input and accumulation in float\n","// Output is float (will be converted to double later if needed)\n","__global__ void wmma_gemm(const half *A, const half *B, float *C,\n","                          int M, int K, int N) {\n","    // Leading dimensions\n","    int lda = K;\n","    int ldb = N;\n","    int ldc = N;\n","\n","    // Each warp computes one 16x16 output tile\n","    // Warp ID within the grid (each warp is an independent unit)\n","    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n","    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n","\n","    // Calculate the starting row and column for this warp's output tile\n","    int warp_row = warpM * WMMA_M;\n","    int warp_col = warpN * WMMA_N;\n","\n","    // Bounds check\n","    if (warp_row >= M || warp_col >= N)\n","        return;\n","\n","    // Declare the fragments\n","    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n","    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n","    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n","\n","    // Initialize accumulator to zero\n","    wmma::fill_fragment(acc_frag, 0.0f);\n","\n","    // Loop over K dimension in steps of WMMA_K\n","    for (int i = 0; i < K; i += WMMA_K) {\n","        if (i < K) {\n","            // Step 1: Load the inputs into fragments\n","            wmma::load_matrix_sync(a_frag, A + warp_row * lda + i, lda);\n","            wmma::load_matrix_sync(b_frag, B + i * ldb + warp_col, ldb);\n","\n","            // Step 2: Perform the matrix multiplication\n","            wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n","        }\n","    }\n","\n","    // Step 3: Store the output\n","    wmma::store_matrix_sync(C + warp_row * ldc + warp_col, acc_frag, ldc, wmma::mem_row_major);\n","}\n","\n","// Convert float to double\n","__global__ void float_to_double(const float *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)input[idx];\n","    }\n","}\n","\n","// CPU reference implementation\n","void cpu_gemm(const double *A, const double *B, double *C, int M, int K, int N) {\n","    for (int i = 0; i < M; i++) {\n","        for (int j = 0; j < N; j++) {\n","            double sum = 0.0;\n","            for (int k = 0; k < K; k++) {\n","                sum += A[i * K + k] * B[k * N + j];\n","            }\n","            C[i * N + j] = sum;\n","        }\n","    }\n","}\n","\n","// Calculate relative error\n","double calculate_error(const double *ref, const double *result, int size) {\n","    double max_error = 0.0;\n","    double sum_error = 0.0;\n","\n","    for (int i = 0; i < size; i++) {\n","        double error = fabs(ref[i] - result[i]);\n","        double rel_error = error / (fabs(ref[i]) + 1e-10);\n","        max_error = fmax(max_error, rel_error);\n","        sum_error += rel_error;\n","    }\n","\n","    return sum_error / size;\n","}\n","\n","void print_wmma_info(int M, int K, int N) {\n","    printf(\"\\n=== WMMA Configuration ===\\n\");\n","    printf(\"Fragment dimensions: M=%d, N=%d, K=%d\\n\", WMMA_M, WMMA_N, WMMA_K);\n","\n","    // Calculate number of fragments needed\n","    int frags_M = (M + WMMA_M - 1) / WMMA_M;\n","    int frags_N = (N + WMMA_N - 1) / WMMA_N;\n","    int frags_K = (K + WMMA_K - 1) / WMMA_K;\n","\n","    printf(\"Number of fragments:\\n\");\n","    printf(\"  M direction: %d fragments (covering %d elements)\\n\", frags_M, M);\n","    printf(\"  N direction: %d fragments (covering %d elements)\\n\", frags_N, N);\n","    printf(\"  K direction: %d fragments (covering %d elements)\\n\", frags_K, K);\n","    printf(\"  Total fragments used: %d\\n\", frags_M * frags_N * frags_K);\n","    printf(\"  Fragment type: half precision (FP16) input, float accumulator\\n\");\n","    printf(\"==========================\\n\\n\");\n","}\n","\n","int main(int argc, char **argv) {\n","    if (argc < 2) {\n","        printf(\"Usage: %s <size> or %s <M> <K> <N>\\n\", argv[0], argv[0]);\n","        printf(\"Example: %s 1024 2048 1024  (for A: 1024x2048, B: 2048x1024)\\n\", argv[0]);\n","        return 1;\n","    }\n","\n","    int M, K, N;\n","    if (argc == 2) {\n","        // Square matrices\n","        M = K = N = atoi(argv[1]);\n","    } else if (argc == 4) {\n","        M = atoi(argv[1]);\n","        K = atoi(argv[2]);\n","        N = atoi(argv[3]);\n","    } else {\n","        fprintf(stderr, \"Invalid arguments\\n\");\n","        return 1;\n","    }\n","\n","    printf(\"Matrix dimensions: A(%d x %d) * B(%d x %d) = C(%d x %d)\\n\\n\",\n","           M, K, K, N, M, N);\n","\n","    // Print WMMA information\n","    print_wmma_info(M, K, N);\n","\n","    // Allocate host memory\n","    size_t bytes_A = M * K * sizeof(double);\n","    size_t bytes_B = K * N * sizeof(double);\n","    size_t bytes_C = M * N * sizeof(double);\n","\n","    double *h_A = (double*)malloc(bytes_A);\n","    double *h_B = (double*)malloc(bytes_B);\n","    double *h_C_cpu = (double*)malloc(bytes_C);\n","    double *h_C_gemm = (double*)malloc(bytes_C);\n","    double *h_C_wmma = (double*)malloc(bytes_C);\n","\n","    // Initialize matrices\n","    for (int i = 0; i < M * K; i++) {\n","        h_A[i] = (double)(rand() % 100) / 10.0;\n","    }\n","    for (int i = 0; i < K * N; i++) {\n","        h_B[i] = (double)(rand() % 100) / 10.0;\n","    }\n","\n","    // CPU computation\n","    printf(\"Running CPU reference...\\n\");\n","    auto cpu_start = std::chrono::high_resolution_clock::now();\n","    cpu_gemm(h_A, h_B, h_C_cpu, M, K, N);\n","    auto cpu_end = std::chrono::high_resolution_clock::now();\n","    std::chrono::duration<double> cpu_duration = cpu_end - cpu_start;\n","    printf(\"CPU time: %.3f ms\\n\\n\", cpu_duration.count() * 1000.0);\n","\n","    // Allocate device memory for double precision\n","    double *d_A, *d_B, *d_C_gemm;\n","    CHECK(cudaMalloc(&d_A, bytes_A));\n","    CHECK(cudaMalloc(&d_B, bytes_B));\n","    CHECK(cudaMalloc(&d_C_gemm, bytes_C));\n","\n","    CHECK(cudaMemcpy(d_A, h_A, bytes_A, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_B, h_B, bytes_B, cudaMemcpyHostToDevice));\n","\n","    // Run tiled GEMM kernel\n","    printf(\"Running tiled GEMM kernel...\\n\");\n","    dim3 block_gemm(TILE_SIZE, TILE_SIZE);\n","    dim3 grid_gemm((N + TILE_SIZE - 1) / TILE_SIZE,\n","                   (M + TILE_SIZE - 1) / TILE_SIZE);\n","\n","    cudaEvent_t start, stop;\n","    CHECK(cudaEventCreate(&start));\n","    CHECK(cudaEventCreate(&stop));\n","\n","    CHECK(cudaEventRecord(start));\n","    gemm<<<grid_gemm, block_gemm>>>(d_A, d_B, d_C_gemm, M, K, N);\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float gemm_time = 0;\n","    CHECK(cudaEventElapsedTime(&gemm_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_gemm, d_C_gemm, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double gemm_error = calculate_error(h_C_cpu, h_C_gemm, M * N);\n","    printf(\"GEMM time: %.3f ms\\n\", gemm_time);\n","    printf(\"GEMM average relative error: %.10f\\n\\n\", gemm_error);\n","\n","    // Allocate device memory for half precision (for WMMA)\n","    half *d_A_half, *d_B_half;\n","    float *d_C_wmma_float;  // WMMA outputs float\n","    double *d_C_wmma_double;  // For final conversion to double\n","    CHECK(cudaMalloc(&d_A_half, M * K * sizeof(half)));\n","    CHECK(cudaMalloc(&d_B_half, K * N * sizeof(half)));\n","    CHECK(cudaMalloc(&d_C_wmma_float, M * N * sizeof(float)));\n","    CHECK(cudaMalloc(&d_C_wmma_double, bytes_C));\n","\n","    // Convert double to half\n","    int threads = 256;\n","    int blocks_A = (M * K + threads - 1) / threads;\n","    int blocks_B = (K * N + threads - 1) / threads;\n","\n","    double_to_half<<<blocks_A, threads>>>(d_A, d_A_half, M * K);\n","    double_to_half<<<blocks_B, threads>>>(d_B, d_B_half, K * N);\n","    CHECK(cudaDeviceSynchronize());\n","\n","    // Run WMMA kernel\n","    printf(\"Running WMMA Tensor Core kernel...\\n\");\n","\n","    // For WMMA: each warp (32 threads) computes one 16x16 output tile\n","    // Use 128 threads per block (4 warps) for good occupancy\n","    // Block dimensions: 128 threads in X (4 warps), 1 in Y\n","    int warps_per_block_x = 4;  // 4 warps per block\n","    dim3 block_wmma(warps_per_block_x * 32, 1);  // 128 threads in X, 1 in Y\n","    dim3 grid_wmma(((M + WMMA_M - 1) / WMMA_M + warps_per_block_x - 1) / warps_per_block_x,\n","                   (N + WMMA_N - 1) / WMMA_N);\n","\n","    printf(\"WMMA Grid: (%d, %d), Block: (%d, %d)\\n\",\n","           grid_wmma.x, grid_wmma.y, block_wmma.x, block_wmma.y);\n","\n","    CHECK(cudaEventRecord(start));\n","    wmma_gemm<<<grid_wmma, block_wmma>>>(d_A_half, d_B_half, d_C_wmma_float, M, K, N);\n","    CHECK(cudaGetLastError());\n","\n","    // Convert float output to double for comparison\n","    int blocks_C = (M * N + threads - 1) / threads;\n","    float_to_double<<<blocks_C, threads>>>(d_C_wmma_float, d_C_wmma_double, M * N);\n","    CHECK(cudaGetLastError());\n","\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float wmma_time = 0;\n","    CHECK(cudaEventElapsedTime(&wmma_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_wmma, d_C_wmma_double, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double wmma_error = calculate_error(h_C_cpu, h_C_wmma, M * N);\n","    printf(\"WMMA time: %.3f ms\\n\", wmma_time);\n","    printf(\"WMMA average relative error: %.10f\\n\", wmma_error);\n","\n","    // Debug: Print first few values\n","    printf(\"DEBUG - First 10 values:\\n\");\n","    printf(\"  CPU:  \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_cpu[i]);\n","    printf(\"\\n  GEMM: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_gemm[i]);\n","    printf(\"\\n  WMMA: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_wmma[i]);\n","    printf(\"\\n\\n\");\n","\n","    // Print performance comparison\n","    printf(\"=== Performance Summary ===\\n\");\n","    printf(\"CPU:  %.3f ms\\n\", cpu_duration.count() * 1000.0);\n","    printf(\"GEMM: %.3f ms (%.2fx speedup over CPU)\\n\",\n","           gemm_time, cpu_duration.count() * 1000.0 / gemm_time);\n","    printf(\"WMMA: %.3f ms (%.2fx speedup over CPU, %.2fx over GEMM)\\n\",\n","           wmma_time, cpu_duration.count() * 1000.0 / wmma_time, gemm_time / wmma_time);\n","    printf(\"\\n=== Accuracy Summary ===\\n\");\n","    printf(\"GEMM avg relative error: %.10f\\n\", gemm_error);\n","    printf(\"WMMA avg relative error: %.10f\\n\", wmma_error);\n","\n","    // Cleanup\n","    CHECK(cudaEventDestroy(start));\n","    CHECK(cudaEventDestroy(stop));\n","    free(h_A);\n","    free(h_B);\n","    free(h_C_cpu);\n","    free(h_C_gemm);\n","    free(h_C_wmma);\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_B));\n","    CHECK(cudaFree(d_C_gemm));\n","    CHECK(cudaFree(d_A_half));\n","    CHECK(cudaFree(d_B_half));\n","    CHECK(cudaFree(d_C_wmma_float));\n","    CHECK(cudaFree(d_C_wmma_double));\n","\n","    return 0;\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1767106124967,"user":{"displayName":"Ade Pramono","userId":"09541809579049427775"},"user_tz":-60},"id":"IRBXFjPyMskM","outputId":"33fa5f5e-1865-4aea-c97c-4d98739bd71a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing wmma_matrixMul8.cu\n"]}],"source":["%%writefile wmma_matrixMul8.cu\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","#include <cuda_fp16.h>\n","#include <mma.h>\n","#include <chrono>\n","#include <cmath>\n","\n","using namespace nvcuda;\n","\n","#define CHECK(call) do {                                 \\\n","    cudaError_t err = (call);                            \\\n","    if (err != cudaSuccess) {                            \\\n","        fprintf(stderr, \"CUDA error: %s (%s:%d)\\n\",      \\\n","                cudaGetErrorString(err), __FILE__, __LINE__); \\\n","        exit(1);                                         \\\n","    }                                                    \\\n","} while (0)\n","\n","// WMMA tile dimensions (for half precision: 16x16x16)\n","#define WMMA_M 16\n","#define WMMA_N 16\n","#define WMMA_K 16\n","\n","// Tiled GEMM kernel (baseline GPU implementation)\n","#define TILE_SIZE 16\n","\n","__global__ void gemm(const double *A, const double *B, double *C,\n","                     int M, int K, int N) {\n","    __shared__ double As[TILE_SIZE][TILE_SIZE];\n","    __shared__ double Bs[TILE_SIZE][TILE_SIZE];\n","\n","    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n","    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n","\n","    double sum = 0.0;\n","\n","    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n","        // Load tiles into shared memory\n","        if (row < M && t * TILE_SIZE + threadIdx.x < K)\n","            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n","        else\n","            As[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        if (col < N && t * TILE_SIZE + threadIdx.y < K)\n","            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n","        else\n","            Bs[threadIdx.y][threadIdx.x] = 0.0;\n","\n","        __syncthreads();\n","\n","        // Compute partial product\n","        for (int k = 0; k < TILE_SIZE; k++) {\n","            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (row < M && col < N) {\n","        C[row * N + col] = sum;\n","    }\n","}\n","\n","// Convert double precision to half precision\n","__global__ void double_to_half(const double *input, half *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = __float2half((float)input[idx]);\n","    }\n","}\n","\n","// Convert half precision to double precision\n","__global__ void half_to_double(const half *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)__half2float(input[idx]);\n","    }\n","}\n","\n","// WMMA kernel for matrix multiplication using Tensor Cores\n","// C (M x N) = A (M x K) * B (K x N)\n","// Using half precision for input and accumulation in float\n","// Output is float (will be converted to double later if needed)\n","__global__ void wmma_gemm(const half *A, const half *B, float *C,\n","                          int M, int K, int N) {\n","    // Leading dimensions\n","    int lda = K;\n","    int ldb = N;\n","    int ldc = N;\n","\n","    // Each warp computes one 16x16 output tile\n","    // Warp ID within the grid (each warp is an independent unit)\n","    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n","    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n","\n","    // Calculate the starting row and column for this warp's output tile\n","    int warp_row = warpM * WMMA_M;\n","    int warp_col = warpN * WMMA_N;\n","\n","    // Bounds check\n","    if (warp_row >= M || warp_col >= N)\n","        return;\n","\n","    // Declare the fragments\n","    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n","    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;\n","    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n","\n","    // Initialize accumulator to zero\n","    wmma::fill_fragment(acc_frag, 0.0f);\n","\n","    // Loop over K dimension in steps of WMMA_K\n","    for (int i = 0; i < K; i += WMMA_K) {\n","        if (i < K) {\n","            // Step 1: Load the inputs into fragments\n","            wmma::load_matrix_sync(a_frag, A + warp_row * lda + i, lda);\n","            wmma::load_matrix_sync(b_frag, B + i * ldb + warp_col, ldb);\n","\n","            // Step 2: Perform the matrix multiplication\n","            wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n","        }\n","    }\n","\n","    // Step 3: Store the output\n","    wmma::store_matrix_sync(C + warp_row * ldc + warp_col, acc_frag, ldc, wmma::mem_row_major);\n","}\n","\n","// Convert float to double\n","__global__ void float_to_double(const float *input, double *output, int size) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (idx < size) {\n","        output[idx] = (double)input[idx];\n","    }\n","}\n","\n","// CPU reference implementation\n","void cpu_gemm(const double *A, const double *B, double *C, int M, int K, int N) {\n","    for (int i = 0; i < M; i++) {\n","        for (int j = 0; j < N; j++) {\n","            double sum = 0.0;\n","            for (int k = 0; k < K; k++) {\n","                sum += A[i * K + k] * B[k * N + j];\n","            }\n","            C[i * N + j] = sum;\n","        }\n","    }\n","}\n","\n","// Calculate relative error\n","double calculate_error(const double *ref, const double *result, int size) {\n","    double max_error = 0.0;\n","    double sum_error = 0.0;\n","\n","    for (int i = 0; i < size; i++) {\n","        double error = fabs(ref[i] - result[i]);\n","        double rel_error = error / (fabs(ref[i]) + 1e-10);\n","        max_error = fmax(max_error, rel_error);\n","        sum_error += rel_error;\n","    }\n","\n","    return sum_error / size;\n","}\n","\n","void print_wmma_info(int M, int K, int N) {\n","    printf(\"\\n=== WMMA Configuration ===\\n\");\n","    printf(\"Fragment dimensions: M=%d, N=%d, K=%d\\n\", WMMA_M, WMMA_N, WMMA_K);\n","\n","    // Calculate number of fragments needed\n","    int frags_M = (M + WMMA_M - 1) / WMMA_M;\n","    int frags_N = (N + WMMA_N - 1) / WMMA_N;\n","    int frags_K = (K + WMMA_K - 1) / WMMA_K;\n","\n","    printf(\"Number of fragments:\\n\");\n","    printf(\"  M direction: %d fragments (covering %d elements)\\n\", frags_M, M);\n","    printf(\"  N direction: %d fragments (covering %d elements)\\n\", frags_N, N);\n","    printf(\"  K direction: %d fragments (covering %d elements)\\n\", frags_K, K);\n","    printf(\"  Total fragments used: %d\\n\", frags_M * frags_N * frags_K);\n","    printf(\"  Fragment type: half precision (FP16) input, float accumulator\\n\");\n","    printf(\"==========================\\n\\n\");\n","}\n","\n","int main(int argc, char **argv) {\n","    if (argc < 2) {\n","        printf(\"Usage: %s <size> or %s <M> <K> <N>\\n\", argv[0], argv[0]);\n","        printf(\"Example: %s 1024 2048 1024  (for A: 1024x2048, B: 2048x1024)\\n\", argv[0]);\n","        return 1;\n","    }\n","\n","    int M, K, N;\n","    if (argc == 2) {\n","        // Square matrices\n","        M = K = N = atoi(argv[1]);\n","    } else if (argc == 4) {\n","        M = atoi(argv[1]);\n","        K = atoi(argv[2]);\n","        N = atoi(argv[3]);\n","    } else {\n","        fprintf(stderr, \"Invalid arguments\\n\");\n","        return 1;\n","    }\n","\n","    printf(\"Matrix dimensions: A(%d x %d) * B(%d x %d) = C(%d x %d)\\n\\n\",\n","           M, K, K, N, M, N);\n","\n","    // Print WMMA information\n","    print_wmma_info(M, K, N);\n","\n","    // Allocate host memory\n","    size_t bytes_A = M * K * sizeof(double);\n","    size_t bytes_B = K * N * sizeof(double);\n","    size_t bytes_C = M * N * sizeof(double);\n","\n","    double *h_A = (double*)malloc(bytes_A);\n","    double *h_B = (double*)malloc(bytes_B);\n","    double *h_C_cpu = (double*)malloc(bytes_C);\n","    double *h_C_gemm = (double*)malloc(bytes_C);\n","    double *h_C_wmma = (double*)malloc(bytes_C);\n","\n","    // Initialize matrices\n","    for (int i = 0; i < M * K; i++) {\n","        h_A[i] = (double)(rand() % 100) / 10.0;\n","    }\n","    for (int i = 0; i < K * N; i++) {\n","        h_B[i] = (double)(rand() % 100) / 10.0;\n","    }\n","\n","    // CPU computation\n","    printf(\"Running CPU reference...\\n\");\n","    auto cpu_start = std::chrono::high_resolution_clock::now();\n","    cpu_gemm(h_A, h_B, h_C_cpu, M, K, N);\n","    auto cpu_end = std::chrono::high_resolution_clock::now();\n","    std::chrono::duration<double> cpu_duration = cpu_end - cpu_start;\n","    printf(\"CPU time: %.3f ms\\n\\n\", cpu_duration.count() * 1000.0);\n","\n","    // Allocate device memory for double precision\n","    double *d_A, *d_B, *d_C_gemm;\n","    CHECK(cudaMalloc(&d_A, bytes_A));\n","    CHECK(cudaMalloc(&d_B, bytes_B));\n","    CHECK(cudaMalloc(&d_C_gemm, bytes_C));\n","\n","    CHECK(cudaMemcpy(d_A, h_A, bytes_A, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_B, h_B, bytes_B, cudaMemcpyHostToDevice));\n","\n","    // Run tiled GEMM kernel\n","    printf(\"Running tiled GEMM kernel...\\n\");\n","    dim3 block_gemm(TILE_SIZE, TILE_SIZE);\n","    dim3 grid_gemm((N + TILE_SIZE - 1) / TILE_SIZE,\n","                   (M + TILE_SIZE - 1) / TILE_SIZE);\n","\n","    cudaEvent_t start, stop;\n","    CHECK(cudaEventCreate(&start));\n","    CHECK(cudaEventCreate(&stop));\n","\n","    CHECK(cudaEventRecord(start));\n","    gemm<<<grid_gemm, block_gemm>>>(d_A, d_B, d_C_gemm, M, K, N);\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float gemm_time = 0;\n","    CHECK(cudaEventElapsedTime(&gemm_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_gemm, d_C_gemm, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double gemm_error = calculate_error(h_C_cpu, h_C_gemm, M * N);\n","    printf(\"GEMM time: %.3f ms\\n\", gemm_time);\n","    printf(\"GEMM average relative error: %.10f\\n\\n\", gemm_error);\n","\n","    // Allocate device memory for half precision (for WMMA)\n","    half *d_A_half, *d_B_half;\n","    float *d_C_wmma_float;  // WMMA outputs float\n","    double *d_C_wmma_double;  // For final conversion to double\n","    CHECK(cudaMalloc(&d_A_half, M * K * sizeof(half)));\n","    CHECK(cudaMalloc(&d_B_half, K * N * sizeof(half)));\n","    CHECK(cudaMalloc(&d_C_wmma_float, M * N * sizeof(float)));\n","    CHECK(cudaMalloc(&d_C_wmma_double, bytes_C));\n","\n","    // Convert double to half\n","    int threads = 256;\n","    int blocks_A = (M * K + threads - 1) / threads;\n","    int blocks_B = (K * N + threads - 1) / threads;\n","\n","    double_to_half<<<blocks_A, threads>>>(d_A, d_A_half, M * K);\n","    double_to_half<<<blocks_B, threads>>>(d_B, d_B_half, K * N);\n","    CHECK(cudaDeviceSynchronize());\n","\n","    // Run WMMA kernel\n","    printf(\"Running WMMA Tensor Core kernel...\\n\");\n","\n","    // For WMMA: each warp (32 threads) computes one 16x16 output tile\n","    // Use 128 threads per block (4 warps) for good occupancy\n","    // Block dimensions: 128 threads in X (4 warps), 1 in Y\n","    int warps_per_block_x = 8;  // 4 warps per block\n","    dim3 block_wmma(warps_per_block_x * 32, 1);  // 128 threads in X, 1 in Y\n","    dim3 grid_wmma(((M + WMMA_M - 1) / WMMA_M + warps_per_block_x - 1) / warps_per_block_x,\n","                   (N + WMMA_N - 1) / WMMA_N);\n","\n","    printf(\"WMMA Grid: (%d, %d), Block: (%d, %d)\\n\",\n","           grid_wmma.x, grid_wmma.y, block_wmma.x, block_wmma.y);\n","\n","    CHECK(cudaEventRecord(start));\n","    wmma_gemm<<<grid_wmma, block_wmma>>>(d_A_half, d_B_half, d_C_wmma_float, M, K, N);\n","    CHECK(cudaGetLastError());\n","\n","    // Convert float output to double for comparison\n","    int blocks_C = (M * N + threads - 1) / threads;\n","    float_to_double<<<blocks_C, threads>>>(d_C_wmma_float, d_C_wmma_double, M * N);\n","    CHECK(cudaGetLastError());\n","\n","    CHECK(cudaEventRecord(stop));\n","    CHECK(cudaEventSynchronize(stop));\n","\n","    float wmma_time = 0;\n","    CHECK(cudaEventElapsedTime(&wmma_time, start, stop));\n","    CHECK(cudaMemcpy(h_C_wmma, d_C_wmma_double, bytes_C, cudaMemcpyDeviceToHost));\n","\n","    double wmma_error = calculate_error(h_C_cpu, h_C_wmma, M * N);\n","    printf(\"WMMA time: %.3f ms\\n\", wmma_time);\n","    printf(\"WMMA average relative error: %.10f\\n\", wmma_error);\n","\n","    // Debug: Print first few values\n","    printf(\"DEBUG - First 10 values:\\n\");\n","    printf(\"  CPU:  \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_cpu[i]);\n","    printf(\"\\n  GEMM: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_gemm[i]);\n","    printf(\"\\n  WMMA: \");\n","    for (int i = 0; i < 10 && i < M * N; i++) printf(\"%.2f \", h_C_wmma[i]);\n","    printf(\"\\n\\n\");\n","\n","    // Print performance comparison\n","    printf(\"=== Performance Summary ===\\n\");\n","    printf(\"CPU:  %.3f ms\\n\", cpu_duration.count() * 1000.0);\n","    printf(\"GEMM: %.3f ms (%.2fx speedup over CPU)\\n\",\n","           gemm_time, cpu_duration.count() * 1000.0 / gemm_time);\n","    printf(\"WMMA: %.3f ms (%.2fx speedup over CPU, %.2fx over GEMM)\\n\",\n","           wmma_time, cpu_duration.count() * 1000.0 / wmma_time, gemm_time / wmma_time);\n","    printf(\"\\n=== Accuracy Summary ===\\n\");\n","    printf(\"GEMM avg relative error: %.10f\\n\", gemm_error);\n","    printf(\"WMMA avg relative error: %.10f\\n\", wmma_error);\n","\n","    // Cleanup\n","    CHECK(cudaEventDestroy(start));\n","    CHECK(cudaEventDestroy(stop));\n","    free(h_A);\n","    free(h_B);\n","    free(h_C_cpu);\n","    free(h_C_gemm);\n","    free(h_C_wmma);\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_B));\n","    CHECK(cudaFree(d_C_gemm));\n","    CHECK(cudaFree(d_A_half));\n","    CHECK(cudaFree(d_B_half));\n","    CHECK(cudaFree(d_C_wmma_float));\n","    CHECK(cudaFree(d_C_wmma_double));\n","\n","    return 0;\n","}\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOeGxxcHM7Ki4eRVKhzOBmb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}