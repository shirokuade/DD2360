{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyO+E2zYzw7c/AaCboToticq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirokuade/DD2360/blob/main/Assignment3/Q2/DD2360_A3_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxVFWXUR999I",
        "outputId": "e283593f-a6bb-4ab4-d64d-7bbf82f4f479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vectorAdd.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile vectorAdd.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cstdio>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "#define CHECK(call) do {                                 \\\n",
        "    cudaError_t err = (call);                            \\\n",
        "    if (err != cudaSuccess) {                            \\\n",
        "        std::fprintf(stderr, \"CUDA error: %s (%s:%d)\\n\", \\\n",
        "                     cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
        "        std::exit(1);                                    \\\n",
        "    }                                                    \\\n",
        "} while (0)\n",
        "\n",
        "/* Our over-simplified CUDA kernel */\n",
        "/* Parallel vector add kernel: c[i] = a[i] + b[i] for i < n */\n",
        "__global__ void add(const int *a, const int *b, int *c, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        c[idx] = a[idx] + b[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    // Problem size (can be overridden by command-line)\n",
        "    int N = 1 << 20; // default 1,048,576 elements\n",
        "    if (argc > 1) {\n",
        "        int v = atoi(argv[1]);\n",
        "        if (v > 0) N = v;\n",
        "    }\n",
        "\n",
        "    size_t bytes = (size_t)N * sizeof(int);\n",
        "\n",
        "    // Allocate in host memory and initialize\n",
        "    int *h_a = (int*)malloc(bytes);\n",
        "    int *h_b = (int*)malloc(bytes);\n",
        "    int *h_c = (int*)malloc(bytes);\n",
        "    if (!h_a || !h_b || !h_c) {\n",
        "        fprintf(stderr, \"Host malloc failed\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_a[i] = i;\n",
        "        h_b[i] = 2*i;\n",
        "        h_c[i] = 0;\n",
        "    }\n",
        "\n",
        "    // Allocate in device memory\n",
        "    int *d_a = nullptr, *d_b = nullptr, *d_c = nullptr;\n",
        "    CHECK(cudaMalloc((void **)&d_a, bytes));\n",
        "    CHECK(cudaMalloc((void **)&d_b, bytes));\n",
        "    CHECK(cudaMalloc((void **)&d_c, bytes));\n",
        "\n",
        "    // Copy from host memory to device memory\n",
        "    CHECK(cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // Initialize thread block and thread grid\n",
        "    int tpb = 128;\n",
        "    int blocks = (N + tpb - 1) / tpb;\n",
        "\n",
        "    // Print launch configuration\n",
        "    long long total_threads = (long long)blocks * (long long)tpb;\n",
        "    long long extra_threads = total_threads - (long long)N;\n",
        "    printf(\"Launching kernel with N=%zu, blocks=%d, threads_per_block=%d, total_threads=%lld, extra_threads=%lld\\n\",\n",
        "        (size_t)N, blocks, tpb, total_threads, extra_threads);\n",
        "\n",
        "    // Invoke the CUDA kernel with GPU timing using CUDA events\n",
        "    cudaEvent_t start_evt, stop_evt;\n",
        "    CHECK(cudaEventCreate(&start_evt));\n",
        "    CHECK(cudaEventCreate(&stop_evt));\n",
        "    CHECK(cudaEventRecord(start_evt, 0));\n",
        "\n",
        "    add<<<blocks, tpb>>>(d_a, d_b, d_c, N);\n",
        "    CHECK(cudaGetLastError());\n",
        "    CHECK(cudaEventRecord(stop_evt, 0));\n",
        "    CHECK(cudaEventSynchronize(stop_evt));\n",
        "\n",
        "    float gpu_ms = 0.0f;\n",
        "    CHECK(cudaEventElapsedTime(&gpu_ms, start_evt, stop_evt));\n",
        "\n",
        "    // Copy result from GPU to CPU\n",
        "    CHECK(cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost));\n",
        "    // Destroy events\n",
        "    CHECK(cudaEventDestroy(start_evt));\n",
        "    CHECK(cudaEventDestroy(stop_evt));\n",
        "\n",
        "    // Compute CPU reference with timing\n",
        "    int *h_ref = (int*)malloc(bytes);\n",
        "    if (!h_ref) {\n",
        "        fprintf(stderr, \"Host malloc for reference failed\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_ref[i] = h_a[i] + h_b[i];\n",
        "    }\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> cpu_d = cpu_end - cpu_start;\n",
        "    double cpu_s = cpu_d.count();\n",
        "\n",
        "    // Compare results\n",
        "    long long mismatches = 0;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        if (h_c[i] != h_ref[i]) {\n",
        "            ++mismatches;\n",
        "            if (mismatches <= 10) {\n",
        "                printf(\"mismatch at %d: gpu=%d cpu=%d\\n\", i, h_c[i], h_ref[i]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    if (mismatches == 0) {\n",
        "        printf(\"All %d results match. OK\\n\", N);\n",
        "    } else {\n",
        "        printf(\"Found %lld mismatches out of %d\\n\", mismatches, N);\n",
        "    }\n",
        "\n",
        "    // Print timings and throughput\n",
        "    double gpu_s = (double)gpu_ms / 1000.0;\n",
        "    double bytes_processed = 3.0 * (double)N * sizeof(int); // read a, read b, write c\n",
        "    double gpu_gb_s = bytes_processed / (gpu_s * 1e9);\n",
        "    double cpu_gb_s = bytes_processed / (cpu_s * 1e9);\n",
        "    double gpu_melems_s = (double)N / (gpu_s * 1e6);\n",
        "    double cpu_melems_s = (double)N / (cpu_s * 1e6);\n",
        "    printf(\"GPU kernel time: %.3f ms, throughput: %.3f GB/s (%.3f Melem/s)\\n\", gpu_ms, gpu_gb_s, gpu_melems_s);\n",
        "    printf(\"CPU reference time: %.3f ms, throughput: %.3f GB/s (%.3f Melem/s)\\n\", cpu_s*1000.0, cpu_gb_s, cpu_melems_s);\n",
        "\n",
        "    free(h_ref);\n",
        "\n",
        "    // Cleanup host and device memory\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    CHECK(cudaFree(d_a));\n",
        "    CHECK(cudaFree(d_b));\n",
        "    CHECK(cudaFree(d_c));\n",
        "\n",
        "    return (mismatches == 0) ? 0 : 2;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ff9a8d7"
      },
      "source": [
        "!nvcc -arch=sm_75 vectorAdd.cu -o vectorAdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the compiled program with Nsight Compute CLI\n",
        "!ncu ./vectorAdd 263149"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS_1z5rWnkq-",
        "outputId": "24559bf4-16f5-4931-dc57-c2793aeca986",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 4547 (/content/vectorAdd)\n",
            "Launching kernel with N=263149, blocks=2056, threads_per_block=128, total_threads=263168, extra_threads=19\n",
            "==PROF== Profiling \"add\" - 0: 0%....50%....100% - 9 passes\n",
            "All 263149 results match. OK\n",
            "GPU kernel time: 297.893 ms, throughput: 0.011 GB/s (0.883 Melem/s)\n",
            "CPU reference time: 0.865 ms, throughput: 3.651 GB/s (304.237 Melem/s)\n",
            "==PROF== Disconnected from process 4547\n",
            "[4547] vectorAdd@127.0.0.1\n",
            "  add(const int *, const int *, int *, int) (2056, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.60\n",
            "    SM Frequency                    Mhz       581.25\n",
            "    Elapsed Cycles                cycle        7,575\n",
            "    Memory Throughput                 %        66.81\n",
            "    DRAM Throughput                   %        66.81\n",
            "    Duration                         us        13.02\n",
            "    L1/TEX Cache Throughput           %        31.87\n",
            "    L2 Cache Throughput               %        28.31\n",
            "    SM Active Cycles              cycle     5,650.30\n",
            "    Compute (SM) Throughput           %        23.92\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   128\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  2,056\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread         263,168\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                                6.42\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           32\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            8\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        84.94\n",
            "    Achieved Active Warps Per SM           warp        27.18\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 15.06%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle       40,021\n",
            "    Total DRAM Elapsed Cycles        cycle      479,232\n",
            "    Average L1 Active Cycles         cycle     5,650.30\n",
            "    Total L1 Elapsed Cycles          cycle      275,016\n",
            "    Average L2 Active Cycles         cycle     7,561.88\n",
            "    Total L2 Elapsed Cycles          cycle      353,408\n",
            "    Average SM Active Cycles         cycle     5,650.30\n",
            "    Total SM Elapsed Cycles          cycle      275,016\n",
            "    Average SMSP Active Cycles       cycle     5,568.38\n",
            "    Total SMSP Elapsed Cycles        cycle    1,100,064\n",
            "    -------------------------- ----------- ------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./vectorAdd 263149"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1uQy8291IXG",
        "outputId": "0aabec28-508a-4b80-f5cf-618583f4709e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==7671== NVPROF is profiling process 7671, command: ./vectorAdd 263149\n",
            "Launching kernel with N=263149, blocks=2056, threads_per_block=128, total_threads=263168, extra_threads=19\n",
            "All 263149 results match. OK\n",
            "GPU kernel time: 0.114 ms, throughput: 27.766 GB/s (2313.845 Melem/s)\n",
            "CPU reference time: 1.197 ms, throughput: 2.639 GB/s (219.925 Melem/s)\n",
            "==7671== Profiling application: ./vectorAdd 263149\n",
            "==7671== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.53%  180.74us         2  90.368us  90.240us  90.496us  [CUDA memcpy HtoD]\n",
            "                   30.48%  84.063us         1  84.063us  84.063us  84.063us  [CUDA memcpy DtoH]\n",
            "                    3.99%  11.008us         1  11.008us  11.008us  11.008us  add(int const *, int const *, int*, int)\n",
            "      API calls:   99.07%  184.08ms         3  61.361ms  73.212us  183.93ms  cudaMalloc\n",
            "                    0.50%  920.05us         3  306.68us  246.77us  348.33us  cudaMemcpy\n",
            "                    0.25%  457.02us         3  152.34us  102.74us  225.80us  cudaFree\n",
            "                    0.08%  150.41us       114  1.3190us     111ns  60.000us  cuDeviceGetAttribute\n",
            "                    0.06%  118.71us         1  118.71us  118.71us  118.71us  cudaLaunchKernel\n",
            "                    0.01%  27.661us         2  13.830us  6.7030us  20.958us  cudaEventCreate\n",
            "                    0.01%  14.850us         2  7.4250us  3.6600us  11.190us  cudaEventRecord\n",
            "                    0.01%  11.758us         1  11.758us  11.758us  11.758us  cuDeviceGetName\n",
            "                    0.01%  11.572us         1  11.572us  11.572us  11.572us  cudaEventSynchronize\n",
            "                    0.00%  6.8570us         1  6.8570us  6.8570us  6.8570us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.9540us         2  1.4770us     545ns  2.4090us  cudaEventDestroy\n",
            "                    0.00%  2.1750us         1  2.1750us  2.1750us  2.1750us  cudaEventElapsedTime\n",
            "                    0.00%  1.2650us         3     421ns     116ns     883ns  cuDeviceGetCount\n",
            "                    0.00%     756ns         2     378ns     227ns     529ns  cuDeviceGet\n",
            "                    0.00%     495ns         1     495ns     495ns     495ns  cuDeviceTotalMem\n",
            "                    0.00%     357ns         1     357ns     357ns     357ns  cudaGetLastError\n",
            "                    0.00%     307ns         1     307ns     307ns     307ns  cuDeviceGetUuid\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuModuleGetLoadingMode\n"
          ]
        }
      ]
    }
  ]
}